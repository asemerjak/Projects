{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO JEST BRUDNOPIS i historia porównywania klasyfikatorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from typing import List\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = f\"\"\"Good. It IS a page turner. You can read this book in one day, two at the most, and the plot drives the whole book. The unreliable narrators (there are two besides the main character) are as unlikable as they are unreliable, and there isn't a nice male in the book. Entirely plot driven; the characters are paper thin. You can figure out who-dunnit by the middle of the book. The ending is weak. I can't imagine what all the fuss is about, except that it is quick and there are lots of twists and turns, and you can't trust anyone to tell the truth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download([\n",
    "     \"names\",\n",
    "     \"stopwords\",\n",
    "     \"averaged_perceptron_tagger\",\n",
    "     \"vader_lexicon\",\n",
    "     \"punkt\",\n",
    "], quiet=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(review: str) -> List[List[str]]:\n",
    "    \"\"\"Function preprocesses the given text. It switches all the letters to lowercase,\n",
    "    deletes the stopwords and divides the review by sentences.\n",
    "\n",
    "    Args:\n",
    "        review (str): Review text in start format\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: List of sentences, divided by words, without stopwords and lowercase\n",
    "    \"\"\"\n",
    "\n",
    "    stopwords: List[str] = nltk.corpus.stopwords.words(\"english\")\n",
    "    end_sentence_chars: List[str] = [\"?\", \".\", \"!\"]\n",
    "\n",
    "    review_tokenized: List[str] = [word.lower() for word in nltk.word_tokenize(sample_review) if word.isalpha() and word not in stopwords]\n",
    "    review_by_sentences = re.split(r'[.?!] ', sample_review) # divide the string into sentences\n",
    "\n",
    "    # review_tokenized: List[List[str]] = [] #list of sentences every one of which is split into lowercase words without stopwords\n",
    "\n",
    "    # for sentence in review_by_sentences:\n",
    "    #     review_tokenized.append([word.lower() for word in nltk.word_tokenize(sentence) if word.isalpha() and word not in stopwords])\n",
    "\n",
    "    return review_by_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good\n",
      "0.4404\n",
      "It IS a page turner\n",
      "0.0\n",
      "You can read this book in one day, two at the most, and the plot drives the whole book\n",
      "0.0\n",
      "The unreliable narrators (there are two besides the main character) are as unlikable as they are unreliable, and there isn't a nice male in the book\n",
      "-0.3252\n",
      "Entirely plot driven; the characters are paper thin\n",
      "0.0\n",
      "You can figure out who-dunnit by the middle of the book\n",
      "0.0\n",
      "The ending is weak\n",
      "-0.4404\n",
      "I can't imagine what all the fuss is about, except that it is quick and there are lots of twists and turns, and you can't trust anyone to tell the truth.\n",
      "-0.1032\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_positive(review: List[List[str]])-> bool:\n",
    "    \"\"\"This function analyses the sentiment of a given review.\n",
    "\n",
    "    Args:\n",
    "        review (List[List[str]): Processed review (tokenized, lowercase and without stopwords, divided by sentences)\n",
    "\n",
    "    Returns:\n",
    "        bool: False if sentiment is negative, True if it is positive\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    for sentence in review:\n",
    "        print(sentence)\n",
    "        print(sia.polarity_scores(sentence)[\"compound\"])\n",
    "    sentences_scores = [sia.polarity_scores(sentence)[\"compound\"] for sentence in review]\n",
    "\n",
    "    return np.mean(sentences_scores) > 0\n",
    "\n",
    "\n",
    "print(is_positive(preprocess(sample_review)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]\n",
    "\n",
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(500)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(500)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]\n",
    "\n",
    "print(type(positive_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.collocations.BigramCollocationFinder object at 0x0000013275AA8FA0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted])\n",
    "\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted])\n",
    "\n",
    "print(type(positive_bigram_finder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    features[\"mean_compound\"] = np.mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = np.mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])\n",
    "\n",
    "print(type(positive_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               wordcount = 8                 pos : neg    =     20.0 : 1.0\n",
      "               wordcount = 7                 pos : neg    =     13.2 : 1.0\n",
      "               wordcount = 5                 pos : neg    =      5.9 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      4.2 : 1.0\n",
      "               wordcount = 4                 pos : neg    =      2.9 : 1.0\n",
      "               wordcount = 1                 neg : pos    =      1.9 : 1.0\n",
      "               wordcount = 3                 pos : neg    =      1.5 : 1.0\n",
      "               wordcount = 2                 pos : neg    =      1.2 : 1.0\n",
      "           mean_positive = 0.06466666666666666    neg : pos    =      1.0 : 1.0\n",
      "           mean_positive = 0.09848780487804877    neg : pos    =      1.0 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_count = 4*len(features) // 5\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)\n",
    "nltk.classify.accuracy(classifier, features[train_count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.75% - BernoulliNB\n",
      "69.50% - ComplementNB\n",
      "69.50% - MultinomialNB\n",
      "73.25% - KNeighborsClassifier\n",
      "68.25% - DecisionTreeClassifier\n",
      "73.25% - RandomForestClassifier\n",
      "78.50% - LogisticRegression\n",
      "77.00% - MLPClassifier\n",
      "75.25% - AdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "train_count = 4*len(features) // 5\n",
    "shuffle(features)\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "    classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "    classifier.train(features[:train_count])\n",
    "    accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "    print(F\"{accuracy:.2%} - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.classify.SklearnClassifier(LogisticRegression())\n",
    "classifier.train(features[:train_count])\n",
    "accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "\n",
    "classifier.classify(extract_features(sample_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04047e32cab0852ac50d320ed95cb8921a7bc08976754055e1dfe16ca6566ca0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
